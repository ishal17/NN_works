{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification\n",
    "\n",
    "#### Given dataset consists of 24x24 images (spectograms of 4 different word recordings). The labels are integers from 0 to 3. There's a total of 7920 examples, 1980 per each class. Goal is to classify images using convolutional neural network (CNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for reading hdf5 file\n",
    "import h5py\n",
    "\n",
    "# for encoding labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# for building CNN\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "\n",
    "# for loading TensorBoard\n",
    "import datetime, os\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data and store it in numpy arrays\n",
    "f = h5py.File('data/db.dog.hdf5', 'r')\n",
    "\n",
    "data_key = list(f.keys())[0]\n",
    "labels_key = list(f.keys())[1]\n",
    "\n",
    "data = f[data_key]\n",
    "labels = f[labels_key]\n",
    "\n",
    "data_arr = np.array(data)\n",
    "labels_arr = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Explore data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7920, 24, 24) (7920,)\n"
     ]
    }
   ],
   "source": [
    "print (data_arr.shape,labels_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7efd353a4e20>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check example\n",
    "plt.imshow(data_arr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254.0\n",
      "231.0\n"
     ]
    }
   ],
   "source": [
    "# check range of pixel values\n",
    "print(np.max(data_arr))\n",
    "print(np.min(data_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data (we get data in [0,1] interval)\n",
    "# and shift it to [-0.5,0.5]\n",
    "data_arr_scaled = (data_arr - np.min(data_arr)) / (np.max(data_arr) - np.min(data_arr)) - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "-0.5\n"
     ]
    }
   ],
   "source": [
    "# check scaled data\n",
    "print(np.max(data_arr_scaled))\n",
    "print(np.min(data_arr_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data as input format\n",
    "n_examples, dim1, dim2 = data_arr_scaled.shape\n",
    "data_arr_scaled = data_arr_scaled.reshape(n_examples, dim1, dim2,1)\n",
    "\n",
    "# encode labels\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(labels_arr)\n",
    "encoded = encoder.transform(labels_arr)\n",
    "labels_one_hot = to_categorical(labels_arr, num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# split 10% from whole data as test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_arr_scaled, labels_one_hot, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7128, 24, 24, 1) (7128, 4)\n",
      "(792, 24, 24, 1) (792, 4)\n"
     ]
    }
   ],
   "source": [
    "# check dimensions\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Build CNN model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As in many image classification problems using CNN seems good idea\n",
    "model = Sequential()\n",
    "\n",
    "# We'll use two sequential layers of convolution + max_pooling\n",
    "model.add(Conv2D(32, (5, 5), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (5, 5), padding=\"valid\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Then one hidden dense layer and finally output with softmax activation\n",
    "model.add(Flatten())\n",
    "model.add(Dense(200, activation=\"relu\"))\n",
    "model.add(Dense(4, activation=\"softmax\"))\n",
    "\n",
    "# Not gonna go in details for other parameters (+ there are plenty of resources in internet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss for multiclass output\n",
    "from tensorflow.keras.losses import categorical_crossentropy \n",
    "\n",
    "# compile model\n",
    "model.compile(loss = categorical_crossentropy, \n",
    "              optimizer = \"adam\", \n",
    "              metrics =['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "179/179 [==============================] - 5s 25ms/step - loss: 1.0673 - accuracy: 0.5111 - val_loss: 0.2208 - val_accuracy: 0.9320\n",
      "Epoch 2/10\n",
      "179/179 [==============================] - 4s 20ms/step - loss: 0.1749 - accuracy: 0.9461 - val_loss: 0.1195 - val_accuracy: 0.9551\n",
      "Epoch 3/10\n",
      "179/179 [==============================] - 4s 20ms/step - loss: 0.0863 - accuracy: 0.9746 - val_loss: 0.0383 - val_accuracy: 0.9930\n",
      "Epoch 4/10\n",
      "179/179 [==============================] - 4s 21ms/step - loss: 0.0334 - accuracy: 0.9919 - val_loss: 0.0230 - val_accuracy: 0.9951\n",
      "Epoch 5/10\n",
      "179/179 [==============================] - 4s 20ms/step - loss: 0.0341 - accuracy: 0.9892 - val_loss: 0.0281 - val_accuracy: 0.9944\n",
      "Epoch 6/10\n",
      "179/179 [==============================] - 4s 20ms/step - loss: 0.0163 - accuracy: 0.9969 - val_loss: 0.0113 - val_accuracy: 0.9965\n",
      "Epoch 7/10\n",
      "179/179 [==============================] - 4s 21ms/step - loss: 0.0068 - accuracy: 0.9988 - val_loss: 0.0100 - val_accuracy: 0.9958\n",
      "Epoch 8/10\n",
      "179/179 [==============================] - 4s 20ms/step - loss: 0.0072 - accuracy: 0.9979 - val_loss: 0.0058 - val_accuracy: 0.9986\n",
      "Epoch 9/10\n",
      "179/179 [==============================] - 4s 20ms/step - loss: 0.0076 - accuracy: 0.9974 - val_loss: 0.0034 - val_accuracy: 0.9993\n",
      "Epoch 10/10\n",
      "179/179 [==============================] - 4s 22ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 0.9993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7efd34892f70>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create TensorBoard callback and train model\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "model.fit(X_train, y_train, \n",
    "          batch_size = 32, \n",
    "          epochs = 10, \n",
    "          validation_split = 0.2, \n",
    "          callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 10014), started 0:27:53 ago. (Use '!kill 10014' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-e37ff7597c59dba6\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-e37ff7597c59dba6\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Test loss: 0.003396371379494667\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# evaluate results for test set\n",
    "score = model.evaluate(X_test, y_test) \n",
    "print('Test loss:', score[0]) \n",
    "print('Test accuracy:', score[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__As we see strong architecture of CNN easily solved our problem with high accuracy.__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
